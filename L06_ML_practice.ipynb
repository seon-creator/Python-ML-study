{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50V2, EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7680)]  # 7.5GB\n",
    "        )\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# 메모리 정리 콜백\n",
    "class MemoryCleanupCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_all_data(base_dir, img_size=(299, 299)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    categories = ['NORMAL', 'PNEUMONIA']\n",
    "    \n",
    "    for category in categories:\n",
    "        class_num = categories.index(category)\n",
    "        for subset in ['train', 'test', 'val']:\n",
    "            subset_dir = os.path.join(base_dir, subset, category)\n",
    "            for img in os.listdir(subset_dir):\n",
    "                try:\n",
    "                    img_path = os.path.join(subset_dir, img)\n",
    "                    img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    new_array = cv2.resize(img_array, img_size)\n",
    "                    images.append(new_array)\n",
    "                    labels.append(class_num)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2):\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size / (1 - test_size), stratify=y_train_val, random_state=42)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"datasets/Pneumonia\" # 폴더 접근 경로\n",
    "X, y = load_all_data(base_dir,)  # X에는 모든 이미지, y에는 모든 이미지에 대한 label종류(Class 종류)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (3513, 299, 299), Train labels shape: (3513,)\n",
      "Validation data shape: (1171, 299, 299), Validation labels shape: (1171,)\n",
      "Test data shape: (1172, 299, 299), Test labels shape: (1172,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data shape: {X_train.shape}, Train labels shape: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, Validation labels shape: {y_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set class distribution:\n",
      "Class 0: 950 (27.04%)\n",
      "Class 1: 2563 (72.96%)\n",
      "\n",
      "Validation set class distribution:\n",
      "Class 0: 316 (26.99%)\n",
      "Class 1: 855 (73.01%)\n",
      "\n",
      "Test set class distribution:\n",
      "Class 0: 317 (27.05%)\n",
      "Class 1: 855 (72.95%)\n"
     ]
    }
   ],
   "source": [
    "def check_class_distribution(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"Class {cls}: {count} ({count/len(y)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTrain set class distribution:\")\n",
    "check_class_distribution(y_train)\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "check_class_distribution(y_val)\n",
    "print(\"\\nTest set class distribution:\")\n",
    "check_class_distribution(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 299, 299, 1).astype('float32') / 255\n",
    "X_val = X_val.reshape(X_val.shape[0], 299, 299, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 299, 299, 1).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# 레이블을 원-핫 인코딩으로 변환\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(299,299,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    Flatten(), # 가장 큰 특징들을 모아, 완전 연결 계층으로 보내기 위해 1차원으로 펼치는 작업\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(.5), # general case: 0.2~0.3\n",
    "    Dense(2, activation='softmax')  # 출력값 2개\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 17:29:31.393028: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 309ms/step - accuracy: 0.7144 - loss: 0.6121 - val_accuracy: 0.7301 - val_loss: 1.3017\n",
      "Epoch 2/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 284ms/step - accuracy: 0.7310 - loss: 0.6085 - val_accuracy: 0.7301 - val_loss: 0.6492\n",
      "Epoch 3/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 281ms/step - accuracy: 0.7322 - loss: 0.5889 - val_accuracy: 0.7301 - val_loss: 0.6431\n",
      "Epoch 4/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 282ms/step - accuracy: 0.7380 - loss: 0.5855 - val_accuracy: 0.7301 - val_loss: 0.6336\n",
      "Epoch 5/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 279ms/step - accuracy: 0.7374 - loss: 0.5843 - val_accuracy: 0.7301 - val_loss: 0.6530\n",
      "Epoch 6/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 280ms/step - accuracy: 0.7373 - loss: 0.5836 - val_accuracy: 0.7301 - val_loss: 0.6499\n",
      "Epoch 7/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 274ms/step - accuracy: 0.7435 - loss: 0.5791 - val_accuracy: 0.7301 - val_loss: 0.6397\n",
      "Epoch 8/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 283ms/step - accuracy: 0.7210 - loss: 0.6000 - val_accuracy: 0.7301 - val_loss: 0.6442\n",
      "Epoch 9/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 287ms/step - accuracy: 0.7309 - loss: 0.5892 - val_accuracy: 0.7301 - val_loss: 0.6514\n",
      "Epoch 10/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 281ms/step - accuracy: 0.7213 - loss: 0.6030 - val_accuracy: 0.7301 - val_loss: 0.6439\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(esg)",
   "language": "python",
   "name": "esg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
